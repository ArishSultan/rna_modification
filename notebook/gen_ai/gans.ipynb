{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T22:33:49.151427Z",
     "start_time": "2024-06-10T22:33:48.370506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ],
   "id": "f2dd983363236c74",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T22:34:31.915471Z",
     "start_time": "2024-06-10T22:34:31.913391Z"
    }
   },
   "cell_type": "code",
   "source": "device = 'mps'",
   "id": "d9c52a05e8523029",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d4e67219e085bb86"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T23:04:44.060755Z",
     "start_time": "2024-06-10T23:04:43.972205Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "\n",
    "# Define the residual block used in the generator and discriminator\n",
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, units, dropout_rate=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dense1 = layers.Dense(units)\n",
    "        self.swish1 = layers.Activation(tf.nn.swish)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dense2 = layers.Dense(units)\n",
    "        self.swish2 = layers.Activation(tf.nn.swish)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.swish1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.swish2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return inputs + x\n",
    "\n",
    "# Generator model\n",
    "class Generator(Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense1 = layers.Dense(256)\n",
    "        self.res_block1 = ResidualBlock(256)\n",
    "        self.res_block2 = ResidualBlock(512)\n",
    "        self.res_block3 = ResidualBlock(1024)\n",
    "        self.dense2 = layers.Dense(21, activation='sigmoid')  # Output layer for sequence of length 21\n",
    "    \n",
    "    def call(self, z, labels):\n",
    "        x = tf.concat([z, labels], axis=-1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense1 = layers.Dense(1024)\n",
    "        self.res_block1 = ResidualBlock(1024)\n",
    "        self.res_block2 = ResidualBlock(512)\n",
    "        self.res_block3 = ResidualBlock(256)\n",
    "        self.dense2 = layers.Dense(1)  # Output layer without activation\n",
    "    \n",
    "    def call(self, x, labels):\n",
    "        x = tf.concat([x, labels], axis=-1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "# WGAN-GP loss functions\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "def gradient_penalty(discriminator, real_data, fake_data, labels):\n",
    "    alpha = tf.random.uniform([real_data.shape[0], 1], 0.0, 1.0)\n",
    "    interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        interpolated_output = discriminator(interpolated, labels)\n",
    "    grads = tape.gradient(interpolated_output, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp\n",
    "\n",
    "# Training step function\n",
    "@tf.function\n",
    "def train_step(real_data, labels, generator, discriminator, generator_optimizer, discriminator_optimizer, batch_size, gp_weight=10):\n",
    "    z_dim = 128  # Latent vector dimension\n",
    "    \n",
    "    # Sample random noise and generate fake data\n",
    "    z = tf.random.normal([batch_size, z_dim])\n",
    "    fake_data = generator(z, labels)\n",
    "\n",
    "    # Discriminator loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        real_output = discriminator(real_data, labels)\n",
    "        fake_output = discriminator(fake_data, labels)\n",
    "        d_loss = discriminator_loss(real_output, fake_output)\n",
    "        gp = gradient_penalty(discriminator, real_data, fake_data, labels)\n",
    "        total_d_loss = d_loss + gp_weight * gp\n",
    "\n",
    "    d_gradients = tape.gradient(total_d_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    # Generator loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake_data = generator(z, labels)\n",
    "        fake_output = discriminator(fake_data, labels)\n",
    "        g_loss = generator_loss(fake_output)\n",
    "\n",
    "    g_gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "    \n",
    "    return total_d_loss, g_loss\n",
    "\n",
    "# Training function\n",
    "def train(dataset, labels, generator, discriminator, generator_optimizer, discriminator_optimizer, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        for real_data, labels_batch in dataset.batch(batch_size):\n",
    "            d_loss, g_loss = train_step(real_data, labels_batch, generator, discriminator, generator_optimizer, discriminator_optimizer, batch_size)\n",
    "        print(f\"Epoch {epoch+1}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n",
    "\n",
    "# Preparing the dataset\n",
    "def preprocess_sequence(sequence):\n",
    "    # Convert RNA sequences to numerical representation\n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'U': 3}\n",
    "    return [mapping[char] for char in sequence]\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = [\n",
    "    (\"CAUGGAGAGAUGUUCUUUACU\", 0), (\"CAUAUCAACUUUUAUUCUCUC\", 0), \n",
    "    (\"AUUAUGAAACUGUUGUGGUGU\", 0), (\"CAAGUCGGCUUUGCUAUAAAC\", 1),\n",
    "    (\"CCCUGGGCAGUAUAGAGACGU\", 1), (\"UUGCCUUCUUUUAAGAGAUGG\", 1), \n",
    "    (\"AAAGCUAGGUUCCAACCUGAA\", 0), (\"UAGCCGGGCAUGGUGGCACAC\", 0),\n",
    "    (\"AGGUUUUAGUUUUUGCUUUAU\", 1), (\"CAGCCUGGGCUAACCAGCAUG\", 0),\n",
    "    (\"CUUCGAGGCUUUUCCCCACUG\", 0), (\"GGUUUGGACAUUGAAAUGGCU\", 1)\n",
    "]\n",
    "\n",
    "sequences, labels = zip(*data)\n",
    "sequences = np.array([preprocess_sequence(seq) for seq in sequences])\n",
    "labels = np.array(labels)\n",
    "\n",
    "# One-hot encode labels\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=2)\n",
    "\n",
    "# Create dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "\n",
    "# Initialize models and optimizers\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0001, beta_1=0.5, beta_2=0.9)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0001, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "# Train the models\n",
    "train(dataset, labels, generator, discriminator, generator_optimizer, discriminator_optimizer, epochs=10000, batch_size=32)\n"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/pr/4495xxw90_g8dgzr4yr3lsyh0000gn/T/ipykernel_6966/1499253138.py\", line 90, in train_step  *\n        fake_data = generator(z, labels)\n    File \"/Users/arish/Workspace/experiments/rna_modification/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/pr/4495xxw90_g8dgzr4yr3lsyh0000gn/T/ipykernel_6966/1499253138.py\", line 38, in call\n        x = tf.concat([z, labels], axis=-1)\n\n    ValueError: Exception encountered when calling Generator.call().\n    \n    \u001B[1mDimension 0 in both shapes must be equal, but are 32 and 12. Shapes are [32] and [12]. for '{{node generator_1_1/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](random_normal, Cast, generator_1_1/concat/axis)' with input shapes: [32,128], [12,2], [] and with computed input tensors: input[2] = <-1>.\u001B[0m\n    \n    Arguments received by Generator.call():\n      • z=tf.Tensor(shape=(32, 128), dtype=float32)\n      • labels=tf.Tensor(shape=(12, 2), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 154\u001B[0m\n\u001B[1;32m    151\u001B[0m discriminator_optimizer \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam(\u001B[38;5;241m0.0001\u001B[39m, beta_1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, beta_2\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;66;03m# Train the models\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscriminator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerator_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscriminator_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 118\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(dataset, labels, generator, discriminator, generator_optimizer, discriminator_optimizer, epochs, batch_size)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m real_data, labels_batch \u001B[38;5;129;01min\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39mbatch(batch_size):\n\u001B[0;32m--> 118\u001B[0m         d_loss, g_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreal_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscriminator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerator_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscriminator_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Discriminator Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00md_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Generator Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mg_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Workspace/experiments/rna_modification/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/var/folders/pr/4495xxw90_g8dgzr4yr3lsyh0000gn/T/__autograph_generated_filec9wnb9zm.py:12\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001B[0;34m(real_data, labels, generator, discriminator, generator_optimizer, discriminator_optimizer, batch_size, gp_weight)\u001B[0m\n\u001B[1;32m     10\u001B[0m z_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m128\u001B[39m\n\u001B[1;32m     11\u001B[0m z \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mnormal, ([ag__\u001B[38;5;241m.\u001B[39mld(batch_size), ag__\u001B[38;5;241m.\u001B[39mld(z_dim)],), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m---> 12\u001B[0m fake_data \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[1;32m     14\u001B[0m     real_output \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(discriminator), (ag__\u001B[38;5;241m.\u001B[39mld(real_data), ag__\u001B[38;5;241m.\u001B[39mld(labels)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n",
      "File \u001B[0;32m~/Workspace/experiments/rna_modification/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Cell \u001B[0;32mIn[2], line 38\u001B[0m, in \u001B[0;36mGenerator.call\u001B[0;34m(self, z, labels)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, z, labels):\n\u001B[0;32m---> 38\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense1(x)\n\u001B[1;32m     40\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mres_block1(x)\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/var/folders/pr/4495xxw90_g8dgzr4yr3lsyh0000gn/T/ipykernel_6966/1499253138.py\", line 90, in train_step  *\n        fake_data = generator(z, labels)\n    File \"/Users/arish/Workspace/experiments/rna_modification/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/pr/4495xxw90_g8dgzr4yr3lsyh0000gn/T/ipykernel_6966/1499253138.py\", line 38, in call\n        x = tf.concat([z, labels], axis=-1)\n\n    ValueError: Exception encountered when calling Generator.call().\n    \n    \u001B[1mDimension 0 in both shapes must be equal, but are 32 and 12. Shapes are [32] and [12]. for '{{node generator_1_1/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](random_normal, Cast, generator_1_1/concat/axis)' with input shapes: [32,128], [12,2], [] and with computed input tensors: input[2] = <-1>.\u001B[0m\n    \n    Arguments received by Generator.call():\n      • z=tf.Tensor(shape=(32, 128), dtype=float32)\n      • labels=tf.Tensor(shape=(12, 2), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "473b53ae2d4ae278"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
