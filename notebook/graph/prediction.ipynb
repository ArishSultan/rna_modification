{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-12T12:15:25.093076Z",
     "start_time": "2024-07-12T12:15:22.245167Z"
    }
   },
   "source": [
    "import RNA\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from main import sequences\n",
    "from src.dataset import load_benchmark_dataset, Species, Modification"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:15:25.114808Z",
     "start_time": "2024-07-12T12:15:25.094301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dataset = load_benchmark_dataset(Species.human, Modification.psi, True)\n",
    "train_dataset = load_benchmark_dataset(Species.human, Modification.psi)"
   ],
   "id": "f052a425b034b398",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T13:35:21.189287Z",
     "start_time": "2024-07-12T13:35:20.963900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from src.dataset import load_benchmark_dataset, Species, Modification\n",
    "import RNA\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "def sequence_to_graph(sequence):\n",
    "    # Generate dot-bracket notation\n",
    "    (dot_bracket, _) = RNA.fold(sequence)\n",
    "    \n",
    "    # Create node features (one-hot encoding of bases)\n",
    "    bases = ['A', 'U', 'G', 'C']\n",
    "    node_features = np.zeros((len(sequence), len(bases) + 1))\n",
    "    for i, base in enumerate(sequence):\n",
    "        node_features[i, bases.index(base)] = 1\n",
    "        node_features[i, -1] = int(i == len(sequence) // 2)  # Mark central nucleotide\n",
    "    \n",
    "    # Create edges\n",
    "    edges = []\n",
    "    stack = []\n",
    "    for i, char in enumerate(dot_bracket):\n",
    "        if i > 0:\n",
    "            edges.append((i-1, i))  # Backbone connections\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                edges.append((stack.pop(), i))  # Base pairs\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    return torch.tensor(node_features, dtype=torch.float), edge_index\n",
    "\n",
    "class RNADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        x, edge_index = sequence_to_graph(sequence[0])\n",
    "        return Data(x=x, edge_index=edge_index, y=torch.tensor([label], dtype=torch.long))\n",
    "\n",
    "# Step 2: Model Definitions\n",
    "class ShallowGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(ShallowGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x[len(x)//2], dim=0)  # Only central node\n",
    "\n",
    "class ComplexGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(ComplexGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 32)\n",
    "        self.conv3 = GCNConv(32, 16)\n",
    "        self.fc = torch.nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.fc(x[len(x)//2])  # Only central node\n",
    "        return F.log_softmax(x, dim=0)\n",
    "\n",
    "# Step 3: Training Function\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Step 4: Evaluation Function\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "# Step 5: Main Pipeline\n",
    "def run_pipeline():\n",
    "    # Load datasets\n",
    "    train_dataset = load_benchmark_dataset(Species.human, Modification.psi)\n",
    "    test_dataset = load_benchmark_dataset(Species.human, Modification.psi, True)\n",
    "\n",
    "    # Create PyTorch Geometric datasets\n",
    "    train_data = RNADataset(train_dataset.samples, train_dataset.targets)\n",
    "    test_data = RNADataset(test_dataset.samples, test_dataset.targets)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "    # Set up device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Initialize models\n",
    "    num_node_features = train_data[0].num_node_features\n",
    "    shallow_gcn = ShallowGCN(num_node_features).to(device)\n",
    "    complex_gcn = ComplexGCN(num_node_features).to(device)\n",
    "\n",
    "    # Set up optimizers\n",
    "    shallow_optimizer = torch.optim.Adam(shallow_gcn.parameters(), lr=0.01)\n",
    "    complex_optimizer = torch.optim.Adam(complex_gcn.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        train(shallow_gcn, train_loader, shallow_optimizer, device)\n",
    "        train(complex_gcn, train_loader, complex_optimizer, device)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            shallow_acc = evaluate(shallow_gcn, test_loader, device)\n",
    "            complex_acc = evaluate(complex_gcn, test_loader, device)\n",
    "            print(f'Epoch {epoch+1}: Shallow GCN Acc: {shallow_acc:.4f}, Complex GCN Acc: {complex_acc:.4f}')\n",
    "\n",
    "    # Final evaluation and classification report\n",
    "    def get_predictions(model, loader, device):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(data).max(1)[1]\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            true_labels.extend(data.y.cpu().numpy())\n",
    "        return np.array(predictions), np.array(true_labels)\n",
    "\n",
    "    shallow_preds, true_labels = get_predictions(shallow_gcn, test_loader, device)\n",
    "    complex_preds, _ = get_predictions(complex_gcn, test_loader, device)\n",
    "\n",
    "    print(\"Shallow GCN Classification Report:\")\n",
    "    print(classification_report(true_labels, shallow_preds))\n",
    "    \n",
    "    print(\"\\nComplex GCN Classification Report:\")\n",
    "    print(classification_report(true_labels, complex_preds))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ],
   "id": "5c0ca2f85ccbce06",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pr/4495xxw90_g8dgzr4yr3lsyh0000gn/T/ipykernel_31698/311914857.py:50: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x, edge_index = sequence_to_graph(sequence[0])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [2], target: [32])",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 163\u001B[0m\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28mprint\u001B[39m(classification_report(true_labels, complex_preds))\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 163\u001B[0m     \u001B[43mrun_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 132\u001B[0m, in \u001B[0;36mrun_pipeline\u001B[0;34m()\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[0;32m--> 132\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshallow_gcn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshallow_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m     train(complex_gcn, train_loader, complex_optimizer, device)\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[6], line 89\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, loader, optimizer, device)\u001B[0m\n\u001B[1;32m     87\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     88\u001B[0m output \u001B[38;5;241m=\u001B[39m model(data)\n\u001B[0;32m---> 89\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnll_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     91\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/Workspace/experiments/rna_modification/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2733\u001B[0m, in \u001B[0;36mnll_loss\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001B[0m\n\u001B[1;32m   2731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2732\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 2733\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnll_loss_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: size mismatch (got input: [2], target: [32])"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:15:25.122782Z",
     "start_time": "2024-07-12T12:15:25.120195Z"
    }
   },
   "cell_type": "code",
   "source": "train_sequences = train_dataset.samples['sequence'].values",
   "id": "57aa491627344984",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
